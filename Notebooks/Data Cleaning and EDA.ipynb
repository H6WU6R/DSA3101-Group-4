{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install skimpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/wenjing/Desktop/DSA3101')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from skimpy import skim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from collections import Counter\n",
    "import warnings\n",
    "from openpyxl import *\n",
    "from datetime import timedelta\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/Online_Sales.csv')\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skim(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop([\"Product_SKU\", \"Product_Description\", \"Product_Category\", \"Delivery_Charges\", \"Coupon_Status\"], axis=1)\n",
    "data[\"Amount\"]=data[\"Avg_Price\"]*data[\"Quantity\"]\n",
    "data = data.drop([\"Quantity\", \"Avg_Price\"], axis=1)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join customer tenure information\n",
    "customer = pd.read_excel('./data/CustomersData.xlsx')\n",
    "data = pd.merge(data, customer, on=\"CustomerID\", how='left')\n",
    "data = data.drop([\"Gender\", \"Location\"], axis=1)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time span of the transactions is 1 year\n",
    "earliest_data = data['Transaction_Date'].min()\n",
    "latest_date = data['Transaction_Date'].max()\n",
    "\n",
    "print(f\"earliest: {earliest_data}\")\n",
    "print(f\"latest: {latest_date}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "if data['Transaction_ID'].duplicated().any():\n",
    "    print(\"id duplicated\")\n",
    "    data = data.drop_duplicates(subset='Transaction_ID', keep='first')\n",
    "else:\n",
    "    print(\"id identical\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Transaction_Date'] = pd.to_datetime(data['Transaction_Date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect distribution of customer spendings\n",
    "spending_by_customer = data.groupby('CustomerID')['Amount'].sum()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(spending_by_customer, bins=30, kde=True)\n",
    "plt.title('Distribution of Total Spent by Clients')\n",
    "plt.xlabel('Total Spent')\n",
    "plt.ylabel('Number of Clients')\n",
    "plt.show()\n",
    "\n",
    "# We can see that the graph is highly skewed to the left\n",
    "# We can see that most customers spend below $10000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect distribution of tenure months\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(data['Tenure_Months'], bins=range(min(data['Tenure_Months']), max(data['Tenure_Months']) + 1), kde=True)\n",
    "plt.title('Distribution of Tenure Months')\n",
    "plt.xlabel('Tenure Months')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# No obvious trend in trenure months of customers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect distribution of number of transactions\n",
    "purchase_counts = data.groupby('CustomerID')['Transaction_ID'].nunique()\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.histplot(purchase_counts, bins=range(1, purchase_counts.max()+1),kde = True)\n",
    "plt.title('Distribution of Purchase Counts per Customer')\n",
    "plt.xlabel('Number of Purchases')\n",
    "plt.ylabel('Number of Customers')\n",
    "plt.show()\n",
    "\n",
    "# We can see that the graph is highly skewed to the left\n",
    "# Most of our customers purchase below 50 times\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reference_date = pd.to_datetime(\"2020-01-01\")\n",
    "\n",
    "grouped = data.groupby('CustomerID').agg(\n",
    "    join_date=('Transaction_Date', 'min'),\n",
    "    last_purchase_date=('Transaction_Date', 'max'),\n",
    "    frequency=('Transaction_ID', 'count'),\n",
    "    monetary_value=('Amount', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "grouped['recency'] = (reference_date - grouped['last_purchase_date']).dt.days\n",
    "grouped['T'] = (reference_date - grouped['join_date']).dt.days\n",
    "\n",
    "print(grouped.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## CLV prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install lifetimes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifetimes import BetaGeoFitter, GammaGammaFitter\n",
    "\n",
    "#penalizer_coef = 0.1\n",
    "\n",
    "# Fit BG/NBD model (predict purchase frequency)\n",
    "bgf = BetaGeoFitter(penalizer_coef=penalizer_coef)\n",
    "bgf.fit(grouped[\"frequency\"], grouped[\"recency\"], grouped[\"T\"], initial_params=initial_params)\n",
    "\n",
    "# Fit Gamma-Gamma model (predict spending per transaction)\n",
    "ggf = GammaGammaFitter(penalizer_coef=penalizer_coef)\n",
    "ggf.fit(grouped[\"frequency\"], grouped[\"monetary_value\"])\n",
    "\n",
    "# Predict the transaction frequency and average amount in 1 year\n",
    "grouped['predicted_transactions'] = bgf.predict(365, grouped['frequency'], grouped['recency'], grouped['T'])\n",
    "grouped['predicted_average_value'] = ggf.conditional_expected_average_profit(grouped['frequency'], grouped['monetary_value'])\n",
    "\n",
    "# CLV in 1 year\n",
    "grouped['predicted_1yr_clv'] = grouped['predicted_transactions'] * grouped['predicted_average_value']\n",
    "print(grouped.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix_pearson = grouped[['frequency','monetary_value','recency','T','predicted_1yr_clv']].corr(method = 'pearson')\n",
    "mask = np.triu(np.ones_like(corr_matrix_pearson, dtype = bool))\n",
    "\n",
    "fig,ax = plt.subplots(figsize = (10, 5.5))\n",
    "sns.heatmap(corr_matrix_pearson, \n",
    "            annot = True, \n",
    "            annot_kws = {'fontsize':5.5, 'fontweight':'bold'},\n",
    "            fmt = '.3f',\n",
    "            linewidths = 0.6,\n",
    "            cmap = 'RdBu_r', \n",
    "            mask = mask, \n",
    "            square = True,\n",
    "            ax = ax)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation = 45, horizontalalignment = 'right')\n",
    "ax.tick_params(labelsize = 7, labelcolor = 'black')\n",
    "ax.set_title(\"Correlation Matrix-Pearson\", fontsize = 10, fontweight = 'bold', color = 'black')\n",
    "fig.show()\n",
    "\n",
    "# From the correlation matrix, we can see that predicted_1yr_clv is not strongly correlated to other variables\n",
    "# Thus, a combination of features should be used to segment the customers \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Customer segmentation by Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = grouped[['recency', 'frequency', 'monetary_value', 'T', 'predicted_1yr_clv']]\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "grouped['cluster'] = kmeans.fit_predict(X)\n",
    "\n",
    "grouped['cluster'].value_counts().plot(kind='bar')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation of customer segments distribution\n",
    "features = X[['recency', 'frequency', 'monetary_value', 'T', 'predicted_1yr_clv']]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(15, 10))\n",
    "fig.suptitle('Distributuion of Features by Cluster')\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "    grouped.boxplot(column=feature, by='cluster', ax=axes[i])\n",
    "    axes[i].set_title(f'{feature} by Cluster')\n",
    "    axes[i].set_xlabel('Cluster')\n",
    "    axes[i].set_ylabel(feature)\n",
    "\n",
    "# Hide the unused subplot (the last one)\n",
    "axes[-1].axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspection of statistics of each cluster\n",
    "cluster_summary = grouped.groupby('cluster').agg({\n",
    "    'T':'mean',\n",
    "    'monetary_value': 'mean',\n",
    "    'frequency': 'mean',\n",
    "    'recency': 'mean',\n",
    "    'predicted_1yr_clv': 'mean'\n",
    "})\n",
    "print(cluster_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Explanation of the clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Cluster 0: Lost\n",
    "\n",
    " #### This group is characterised by the longest time till their last purchase and lowest average frequency of purchase, thus is deemed to be lost.\n",
    "\n",
    "\n",
    "\n",
    " ### Cluster 1: Loyal Customers\n",
    "\n",
    " #### This group is characterised by the highest average purchase value, relatively recent purchases and high CLV, indicating they are a group of relatively stable and regular customers.\n",
    "\n",
    "\n",
    "\n",
    " ### Cluster 2: Potential Loyalties\n",
    "\n",
    " #### This group is characterised by relatively high average purchase value and frequency, indicating their potential to upgrade to loyal customers.\n",
    "\n",
    "\n",
    "\n",
    " ### Cluster 3: Active customers (highest clv and recency)\n",
    "\n",
    " #### This group is characterised by the shortest tenure length and most recent purchases, indicating they are converted recently and actively making purchases.\n",
    "\n",
    "\n",
    "\n",
    " ### Cluster 4: At Risk\n",
    "\n",
    " #### This group is characterised by relatively long duration since their last purchase, meaning they might be lost without proper maintainance measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Maximise Campaign ROI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 1. Budget and resource allocation\n",
    "\n",
    " #### Basded on customer previous transactions, banks can predict the cusotmer lifetime value. Thus, banks can allocate the cost of acquiring a customer based on the potential value of acquirng this customer to maximise the overall campaign ROI.\n",
    "\n",
    "\n",
    "\n",
    " ### 2. Targeted campaign strategies -- Customer-based strategy\n",
    "\n",
    " #### Basded on previous transaction trends, preferences in certain product category and/or responsiveness to campaigns, banks can design more personalized marketing campaigns targeted at each customer. For example, give exclusive welcome offers to active customers, VIP services or early access to new products to loyal customers, and re-engage the at risk customers before they are lost by enticing them with product categories that they are interested in. This can ensure maximised campaign returns, thus ROI.\n",
    "\n",
    "\n",
    "\n",
    " ### 3. Identifying potential interested groups of a campaign -- Product-based strategy\n",
    "\n",
    " #### When trying to launch a product or campaign, banks can also identify who are the potential audience based on their past purchases, responsiveness to campaigns and recent views. As such, banks can deliver advertisements to its target audience accurately to achieve the largest return."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
