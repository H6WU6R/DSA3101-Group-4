# -*- coding: utf-8 -*-
"""Recommendation system.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xDAxwwaoDa0hAyrPJsrHAH39CG3Lpr-c
"""

import pandas as pd
import numpy as np
import json
from scipy.sparse import csr_matrix, identity
from sklearn.model_selection import train_test_split, KFold
from sklearn.preprocessing import StandardScaler,PolynomialFeatures
from lightfm import LightFM
from lightfm.evaluation import precision_at_k, auc_score
from sklearn.metrics import accuracy_score, f1_score
import itertools
import csv
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.pipeline import Pipeline
from sklearn.impute import KNNImputer
from sklearn.metrics import mean_squared_error
from sklearn.decomposition import PCA

cards = pd.read_csv('/Users/manyuhaochi/Desktop/DSA3101/archive-2/cards_data.csv')
transactions = pd.read_csv('/Users/manyuhaochi/Desktop/DSA3101/archive-2/transactions_data.csv')
users = pd.read_csv('/Users/manyuhaochi/Desktop/DSA3101/archive-2/users_data.csv')

cards.head()

transactions.head()

users.head()

cards['credit_limit']= cards['credit_limit'].replace({'\$': ''}, regex=True).astype(float)
cards.drop(columns=cards.loc[:,'acct_open_date':'card_on_dark_web'], inplace=True)
cards.drop(columns=cards.loc[:,'card_number':'num_cards_issued'], inplace=True)
cards.head()

# print(cards.shape) #(6146, 13)
missing_values = cards.isnull().sum()
missing_values

cards_new = cards.groupby(['client_id', 'card_type'])['credit_limit'].sum().reset_index()
cards_new = cards_new.pivot(index='client_id', columns='card_type', values='credit_limit').fillna(0).reset_index()
#cards_new.drop('card_type',axis = "index", inplace=True)
cards_new.head()

print(cards_new.columns)
print(cards_new.columns.name)
cards_new.rename_axis(None, axis='columns', inplace=True)
cards_new.head()

users.head()
# print(users.shape)#(2000, 14)
missing_values = users.isnull().sum()
missing_values

users.drop(columns=['address','per_capita_income'], inplace=True)
users.loc[:,"yearly_income":"total_debt"]=users.loc[:,"yearly_income":"total_debt"].apply(lambda x: x.replace({'\$': ''}, regex=True).astype(float))
users.head()

users['gender'] = users['gender'].apply(lambda x: 1 if x == 'Male' else 0)
users.head()

final_df = pd.merge(users, cards_new, left_on='id', right_on='client_id', how='inner').sort_values('id').reset_index(drop=True)
final_df.head()

correlation_matrix = final_df.corr()

plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap="coolwarm")
plt.title("Correlation Matrix")
plt.show()

final_df.drop(columns=['client_id','birth_year'], inplace=True)

final_df.head()

# print(transactions.shape) #(13305915, 12)
missing_values = transactions.isnull().sum()
missing_values

"""## select lastest year data"""

transactions["date"] = pd.to_datetime(transactions["date"])
start_date = "2018-11-01"
end_date = "2019-11-01"

tran_df = transactions[(transactions["date"] >= start_date) & (transactions["date"] <= end_date)]

tran_df.head(10)

"""## Remove errorneous transactions"""

tran_df = tran_df[tran_df['errors'].isna()]
tran_df.shape

# Load the JSON file
with open("/Users/manyuhaochi/Desktop/DSA3101/archive-2/mcc_codes.json", "r") as f:
    data = json.load(f)
data

subgroup_mapping = {"Freight & Trucking": ["3730", "4214"],
    "Steel & Metal Products": ["3000", "3001", "3005", "3006", "3007", "3008", "3009",
                                 "3359", "3387", "3389", "3390", "3393", "3395", "3405", "5094"],
    "Retail Stores": ["5300", "5310", "5311", "5411", "5499","5193","3260"],
    "Digital Goods & Computers": ["5815", "5816", "5045"],
    "Utilities & Home Services": ["4900", "7210", "7349", "1711","5719","5712","3174","3144"],

    "Machinery & Tools": ["3058", "3066", "3075", "3504", "3509", "3596","5211","5261","3256"],

    "Rail & Bus Transport": ["3722", "3771", "3775", "4111", "4112", "4121", "4131", "4784", "3780"],

    "Telecommunications & Media": ["4814", "4829", "4899"],

    "Electronics & Appliances": ["3640","3684","5732","5251","5722"],
    "Automotive & Transportation Services": ["5533", "5541", "7531", "7538", "7542", "7549"],

    "Restaurants & Eating Places": ["5812", "5813", "5814"],

    "Clothing & Fashion": ["5621", "5651", "5655", "5661", "5977", "5932", "5947", "7230","3132"],

    "Movies & Theaters": ["7832", "7922", "5192", "5942"],
    "Sports & Recreational Activities": ["7801", "7802", "5941", "5970","7996","7995","5733"],

    "Medical & Healthcare Services": ["8011", "8021", "8041", "8043", "8049", "8062", "8099", "5912", "5921"],

    "Legal & Financial Services": ["8111", "8931", "7276", "7393","6300"],

    "Hotels & Accommodation": ["7011", "4411", "4511", "4722"],

    "Postal Services - Government Only": ["9402"],
}

# Create a reverse lookup: map each code to its category (subgroup)
code_to_category = {}
for category, codes in subgroup_mapping.items():
    for code in codes:
        code_to_category[code] = category

# Step 3: Prepare the CSV data
# The CSV will have three columns: Code, Product, Category
csv_rows = []
header = ["Code", "Product", "Category"]
csv_rows.append(header)

for code, product in data.items():
    # Use the category from our mapping; if a code isn't found, assign "Uncategorized"
    category = code_to_category.get(code, "Uncategorized")
    csv_rows.append([code, product, category])

# Step 4: Write the CSV file
with open("output.csv", "w", newline="", encoding="utf-8") as csvfile:
    writer = csv.writer(csvfile)
    writer.writerows(csv_rows)

print("CSV file 'output.csv' has been created with assigned categories.")

output = pd.read_csv("/Users/manyuhaochi/Downloads/output.csv")

merged_trans = pd.merge(tran_df, output, left_on='mcc', right_on='Code', how='left')
merged_trans.value_counts('client_id')

merged_trans['amount'] = merged_trans['amount'].replace({'\$': ''}, regex=True).astype(float)
category_spend = merged_trans.groupby(['client_id', 'Category'])['amount'].sum().reset_index()
category_spend.head()

spending = category_spend.pivot(index='client_id', columns='Category', values='amount').fillna(0).reset_index()
spending.head()

final = pd.merge(final_df, spending, left_on='id', right_on='client_id', how='left')

final.drop(columns=['client_id','id'], inplace=True)
final.head()

def evaluate_knn_imputer_scaled_transform_original(n_neighbors, data, target_cols, mask_frac=0.1, random_state=42):
    mse_list = []
    # Work on a copy so that modifications for one column do not affect the others.
    data_copy = data.copy()
    np.random.seed(random_state)

    for col in target_cols:
        non_missing_idx = data_copy[data_copy[col].notnull()].index
        if len(non_missing_idx) == 0:
            continue

        # Randomly mask a fraction of non-missing values
        mask_idx = np.random.choice(non_missing_idx,
                                     size=int(mask_frac * len(non_missing_idx)),
                                     replace=False)
        original_values = data_copy.loc[mask_idx, col].copy()
        data_copy.loc[mask_idx, col] = np.nan

        # Build a pipeline: Standardize then impute
        pipeline = Pipeline([
            ('scaler', StandardScaler()),
            ('imputer', KNNImputer(n_neighbors=n_neighbors))
        ])

        imputed_array = pipeline.fit_transform(data_copy)
        imputed_df = pd.DataFrame(imputed_array, columns=data_copy.columns, index=data_copy.index)

        # Retrieve the fitted scaler to manually scale original values
        fitted_scaler = pipeline.named_steps['scaler']
        col_index = list(data_copy.columns).index(col)

        # Scale the original (masked) values using the fitted scaler parameters
        original_values_scaled = (original_values - fitted_scaler.mean_[col_index]) / fitted_scaler.scale_[col_index]
        imputed_values_scaled = imputed_df.loc[mask_idx, col]

        mse = mean_squared_error(original_values_scaled, imputed_values_scaled)
        mse_list.append(mse)

        # Restore original values so the next iteration works on unmodified data_copy
        data_copy.loc[mask_idx, col] = original_values

    return np.mean(mse_list)

target_cols = ['Automotive & Transportation Services', 'Clothing & Fashion',
       'Digital Goods & Computers', 'Electronics & Appliances',
       'Freight & Trucking', 'Hotels & Accommodation',
       'Legal & Financial Services', 'Machinery & Tools',
       'Medical & Healthcare Services', 'Movies & Theaters',
       'Postal Services - Government Only', 'Rail & Bus Transport',
       'Restaurants & Eating Places', 'Retail Stores',
       'Sports & Recreational Activities', 'Steel & Metal Products',
       'Telecommunications & Media', 'Utilities & Home Services']

# List of feature columns used for imputation
feature_cols = ['current_age', 'retirement_age', 'birth_month', 'gender',
       'latitude', 'longitude', 'yearly_income', 'total_debt', 'credit_score',
       'num_credit_cards', 'Credit', 'Debit', 'Debit (Prepaid)']

# Create a working DataFrame with both feature and target columns.
# (Assuming you have a DataFrame called 'final' already loaded.)
cols_for_impute = feature_cols + target_cols
df_impute = final[cols_for_impute].copy()

# Evaluate a range of n_neighbors (e.g., 2 to 10) and record the MSE for each.
results_scaled = {}
ks = list(range(2, 11))
for k in ks:
    mse = evaluate_knn_imputer_scaled_transform_original(k, df_impute, target_cols, mask_frac=0.1, random_state=42)
    results_scaled[k] = mse
    print(f"n_neighbors = {k}, MSE (standardized) = {mse}")

# Plot the elbow curve
plt.figure(figsize=(8, 5))
plt.plot(ks, [results_scaled[k] for k in ks], marker='o')
plt.xlabel('n_neighbors')
plt.ylabel('Average MSE (scaled)')
plt.title('Elbow Plot for KNN Imputer')
plt.grid(True)
plt.show()

# Select the best n_neighbors (with the lowest MSE)
best_n = 5
print("Best n_neighbors (standardized):", best_n)

# Build final pipeline and impute missing values using the best n_neighbors.
pipeline_final = Pipeline([
    ('scaler', StandardScaler()),
    ('imputer', KNNImputer(n_neighbors=best_n))
])
imputed_array = pipeline_final.fit_transform(df_impute)
# Save the imputed & scaled DataFrame.
df_imputed_scaled = pd.DataFrame(imputed_array, columns=df_impute.columns, index=df_impute.index)
print("Head of imputed and scaled data:")
print(df_imputed_scaled.head())

"""## convert back to original scale"""

scaler = pipeline_final.named_steps['scaler']
# Inverse-transform the imputed scaled data.
df_imputed_original_array = scaler.inverse_transform(df_imputed_scaled)
df_imputed_original = pd.DataFrame(df_imputed_original_array, columns=df_impute.columns, index=df_impute.index)
print("Head of imputed data in original scale:")
df_imputed_original.head(10)

income_median = df_imputed_original['yearly_income'].median()
num_cards_median = df_imputed_original['num_credit_cards'].median()
credit_upper = df_imputed_original['credit_score'].quantile(0.75)
income_80 = df_imputed_original['yearly_income'].quantile(0.80)

# For each spending category used in the rules, compute its 75th percentile.
spending_categories = [
    'Retail Stores', 'Restaurants & Eating Places', 'Clothing & Fashion',
    'Movies & Theaters', 'Sports & Recreational Activities', 'Freight & Trucking',
    'Medical & Healthcare Services', 'Postal Services - Government Only',
    'Digital Goods & Computers', 'Telecommunications & Media', 'Utilities & Home Services',
    'Automotive & Transportation Services', 'Steel & Metal Products', 'Machinery & Tools',
    'Rail & Bus Transport', 'Hotels & Accommodation', 'Legal & Financial Services'
]
spending_upper = {cat: df_imputed_original[cat].quantile(0.75) for cat in spending_categories}

def label_rewards_credit_card(row):
    score = 0
    cats = ['Retail Stores', 'Restaurants & Eating Places', 'Clothing & Fashion',
            'Movies & Theaters', 'Sports & Recreational Activities']
    if any(row[cat] >= spending_upper[cat] for cat in cats):
        score += 1
    if row['credit_score'] >= credit_upper:
        score += 1
    if row['num_credit_cards'] >= num_cards_median:
        score += 1
    return min(score, 3)

def label_insurance_solutions(row):
    score = 0
    cats = ['Freight & Trucking', 'Medical & Healthcare Services', 'Postal Services - Government Only']
    if any(row[cat] >= spending_upper[cat] for cat in cats):
        score += 1
    credit_median = df_imputed_original['credit_score'].quantile(0.50)
    credit_80 = df_imputed_original['credit_score'].quantile(0.80)
    if credit_median < row['credit_score'] <= credit_80:
        score += 1
    if row['yearly_income'] >= income_median:
        score += 1
    return min(score, 3)

# 3. Digital Financing:
#    Conditions: (a) any spending category above its upper quantile,
#                (b) yearly income above median,
#                (c) credit score above upper quantile.
def label_digital_financing(row):
    score = 0
    cats = ['Digital Goods & Computers', 'Telecommunications & Media']
    if any(row[cat] >= spending_upper[cat] for cat in cats):
        score += 1
    if row['yearly_income'] >= income_median:
        score += 1
    if row['credit_score'] >= credit_upper:
        score += 1
    return min(score, 3)

# 4. Home Improvement Loan (loan-related):
#    Conditions: Credit score must exceed upper quantile (else 0),
#                (a) spending in Utilities & Home Services above its upper quantile,
#                (b) yearly income above median.
def label_home_improvement_loan(row):
    score = 0
    if row['credit_score'] >= credit_upper:
        score += 1
    if row['Utilities & Home Services'] >= spending_upper['Utilities & Home Services']:
        score += 1
    if row['yearly_income'] >= income_median:
        score += 1
    return min(score, 3)

# 5. Auto & Vehicle Financing (loan-related):
#    Conditions: Credit score must exceed upper quantile,
#                (a) spending in Automotive & Transportation Services above its upper quantile,
#                (b) yearly income above median.
def label_auto_vehicle_financing(row):
    score = 0 # Base point for meeting credit requirement
    if row['credit_score'] >= credit_upper:
        score += 1
    if row['Automotive & Transportation Services'] >= spending_upper['Automotive & Transportation Services']:
        score += 1
    if row['yearly_income'] >= income_median:
        score += 1
    return min(score, 3)

# 6. Commodity & Investment Services:
#    Conditions: (a) any spending category (Steel & Metal Products or Machinery & Tools) above its upper quantile,
#                (b) yearly income above median,
#                (c) low total debt relative to income (total_debt < 0.5*yearly_income).
def label_commodity_investment_services(row):
    score = 0
    cats = ['Steel & Metal Products', 'Machinery & Tools']
    if any(row[cat] >= spending_upper[cat] for cat in cats):
        score += 1
    if row['yearly_income'] >= income_80:
        score += 1
    if row['total_debt'] <= 0.5 * row['yearly_income']:
        score += 1
    return min(score, 3)

# 7. Travel Rewards Card:
#    Conditions: (a) any spending category (Rail & Bus Transport or Hotels & Accommodation) above its upper quantile,
#                (b) yearly income above median,
#                (c) number of credit cards above median.
def label_travel_rewards_card(row):
    score = 0
    cats = ['Rail & Bus Transport', 'Hotels & Accommodation']
    if any(row[cat] >= spending_upper[cat] for cat in cats):
        score += 1
    if row['yearly_income'] >= income_median:
        score += 1
    if row['num_credit_cards'] >= num_cards_median:
        score += 1
    return min(score, 3)

# 8. Savings/Investment Plans:
#    Conditions remain unchanged.
def label_savings_investment_plans(row):
    score = 0
    if row['Debit'] >= 1.5 * row['Credit']:
        score += 1
    if row['total_debt'] < 0.5 * row['yearly_income']:
        score += 1
    if row['retirement_age'] - row['current_age'] <= 15:
        score += 1
    return min(score, 3)

# 9. Wealth Management & Savings:
#    Conditions: (a) spending in Legal & Financial Services above its upper quantile,
#                (b) yearly income above median and low total debt,
#                (c) credit score above upper quantile.
def label_wealth_management_savings(row):
    score = 0
    if row['Legal & Financial Services'] >= spending_upper['Legal & Financial Services']:
        score += 1
    if row['yearly_income'] >= income_80 and row['total_debt'] < 0.5 * row['yearly_income']:
        score += 1
    if row['credit_score'] >= credit_upper:
        score += 1
    return min(score, 3)

# Apply label functions to construct new label columns.
df_imputed_original['Label_Rewards_Credit_Card'] = df_imputed_original.apply(label_rewards_credit_card, axis=1)
df_imputed_original['Label_Insurance_Solutions'] = df_imputed_original.apply(label_insurance_solutions, axis=1)
df_imputed_original['Label_Digital_Financing'] = df_imputed_original.apply(label_digital_financing, axis=1)
df_imputed_original['Label_Home_Improvement_Loan'] = df_imputed_original.apply(label_home_improvement_loan, axis=1)
df_imputed_original['Label_Auto_Vehicle_Financing'] = df_imputed_original.apply(label_auto_vehicle_financing, axis=1)
df_imputed_original['Label_Commodity_Investment_Services'] = df_imputed_original.apply(label_commodity_investment_services, axis=1)
df_imputed_original['Label_Travel_Rewards_Card'] = df_imputed_original.apply(label_travel_rewards_card, axis=1)
df_imputed_original['Label_Savings_Investment_Plans'] = df_imputed_original.apply(label_savings_investment_plans, axis=1)
df_imputed_original['Label_Wealth_Management_Savings'] = df_imputed_original.apply(label_wealth_management_savings, axis=1)

print("Sample label counts:")
df_imputed_original[['Label_Rewards_Credit_Card', 'Label_Insurance_Solutions',
                           'Label_Digital_Financing', 'Label_Home_Improvement_Loan',
                           'Label_Auto_Vehicle_Financing', 'Label_Commodity_Investment_Services',
                           'Label_Travel_Rewards_Card', 'Label_Savings_Investment_Plans',
                           'Label_Wealth_Management_Savings']].head()

def compute_top3_accuracy_for_fold(model, X_val, interactions_val, item_features, k=3):
    num_users = X_val.shape[0]
    top3_acc = []
    users_with_purchases = 0  # Track users who actually bought something

    for user_id in range(num_users):
        # Predict scores for all items for this user
        scores = model.predict(user_id, np.arange(interactions_val.shape[1]),
                             user_features=X_val,
                             item_features=item_features)

        # Get top k predicted item indices
        top3_indices = np.argsort(-scores)[:k]

        # Get true purchased item indices
        true_positives = set(np.where(interactions_val[user_id].toarray().flatten() == 1)[0])
        n_true = len(true_positives)

        # Skip users with no purchases (do not count them in accuracy)
        if n_true == 0:
            continue

        users_with_purchases += 1

        # Case 1: User purchased 3+ items
        if n_true >= 3:
            count = len(set(top3_indices).intersection(true_positives))
            top3_acc.append(count / 3.0)

        # Case 2: User purchased exactly 2 items
        elif n_true == 2:
            intersection = set(top3_indices).intersection(true_positives)
            if len(intersection) == 2:
                top3_acc.append(1.0)
            elif len(intersection) == 1:
                top3_acc.append(0.5)
            else:
                top3_acc.append(0)

        # Case 3: User purchased exactly 1 item
        elif n_true == 1:
            if top3_indices[0] in true_positives:
                top3_acc.append(1.0)
            elif len(set(top3_indices[1:]).intersection(true_positives)) > 0:
                top3_acc.append(0.5)
            else:
                top3_acc.append(0)

    # Return 0 if no users made purchases (edge case)
    if users_with_purchases == 0:
        return 0.0

    return np.mean(top3_acc)

def grid_search_cv(feature_list, X_train_full, Y_train_bin):
    # Extract the features from X_train_full based on the provided feature list.
    X_train_features = X_train_full[feature_list].copy()
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train_features)
    user_features = csr_matrix(X_train_scaled)

    # Build the interaction matrix from Y_train_bin.
    interactions = csr_matrix(Y_train_bin.values)
    num_items = interactions.shape[1]
    item_features = identity(num_items, format='csr')

    # Define hyperparameter grid.
    param_grid = {
        'loss': ['warp', 'bpr','logistic'],
        'no_components': [16, 32, 64],
        'learning_rate': [0.001, 0.01, 0.05],
        'epochs': [30, 50],
        'user_alpha': [1e-5, 1e-4],
        'item_alpha': [1e-5, 1e-4]
    }

    # Set up 5-fold CV.
    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    grid_results = []
    upsample_factor = 2

    for loss, no_components, learning_rate, epochs, user_alpha, item_alpha in itertools.product(
        param_grid['loss'],
        param_grid['no_components'],
        param_grid['learning_rate'],
        param_grid['epochs'],
        param_grid['user_alpha'],
        param_grid['item_alpha']
    ):
        fold_top3_acc = []
        fold_prec = []

        for train_idx, val_idx in kf.split(user_features):
            X_train_cv = user_features[train_idx]
            X_val_cv = user_features[val_idx]

            # Get the training interactions for this fold.
            fold_train = interactions[train_idx].toarray().astype(float)

            # Upsample the minority class for each product column in the training fold.
            for j in range(fold_train.shape[1]):
                pos_count = np.sum(fold_train[:, j] == 1)
                neg_count = np.sum(fold_train[:, j] == 0)
                if pos_count / fold_train.shape[0] < 0.3:
                    fold_train[:, j] = np.where(fold_train[:, j] == 1,
                                                fold_train[:, j] * upsample_factor,
                                                fold_train[:, j])
                elif neg_count / fold_train.shape[0] < 0.3:
                    fold_train[:, j] = np.where(fold_train[:, j] == 0,
                                                fold_train[:, j] * upsample_factor,
                                                fold_train[:, j])
            fold_train_sparse = csr_matrix(fold_train)

            # Validation interactions (untouched).
            fold_val = interactions[val_idx].toarray()
            fold_val_sparse = csr_matrix(fold_val)

            # Train LightFM on this fold.
            model_cv = LightFM(loss=loss, no_components=no_components,
                               learning_rate=learning_rate,
                               user_alpha=user_alpha,
                               item_alpha=item_alpha,
                               random_state=42)
            model_cv.fit(fold_train_sparse,
                         user_features=X_train_cv,
                         item_features=item_features,
                         epochs=epochs,
                         num_threads=4)

            # Standard precision@3.
            prec = precision_at_k(model_cv, fold_val_sparse,
                                  user_features=X_val_cv,
                                  item_features=item_features,
                                  k=3).mean()
            # Compute custom top-3 accuracy using our function.
            top3_acc = compute_top3_accuracy_for_fold(model_cv, X_val_cv, fold_val_sparse, item_features, k=3)

            fold_top3_acc.append(top3_acc)
            fold_prec.append(prec)

        avg_top3_acc = np.mean(fold_top3_acc)
        avg_prec = np.mean(fold_prec)

        grid_results.append({
            'loss': loss,
            'no_components': no_components,
            'learning_rate': learning_rate,
            'epochs': epochs,
            'user_alpha': user_alpha,
            'item_alpha': item_alpha,
            'top3_accuracy': avg_top3_acc,
            'precision@3': avg_prec
        })

        print(f"Params: loss={loss}, components={no_components}, "
              f"lr={learning_rate}, epochs={epochs}, user_alpha={user_alpha}, item_alpha={item_alpha} -> "
              f"Top3 Accuracy: {avg_top3_acc:.4f}, Precision@3: {avg_prec:.4f}")

    best_params = max(grid_results, key=lambda x: x['top3_accuracy'])
    return best_params, grid_results

# --------------------------------------------------------------------
# Custom function to compute top-3 accuracy for a fold using our rule:
#   - If user has 3 or more purchases: accuracy = (# predicted in top3) / 3.
#   - If user has exactly 2 purchases: 1 if both in top3, 0.5 if one is, else 0.
#   - If user has exactly 1 purchase: 1 if top prediction is correct, 0.5 if in position 2 or 3, else 0.
#   - For users with no purchase, we return 0.
# --------------------------------------------------------------------
# Binarize the ordinal labels: define a "positive" interaction if label >= 2.

label_cols = ['Label_Rewards_Credit_Card', 'Label_Insurance_Solutions',
              'Label_Digital_Financing', 'Label_Home_Improvement_Loan',
              'Label_Auto_Vehicle_Financing', 'Label_Commodity_Investment_Services',
              'Label_Travel_Rewards_Card', 'Label_Savings_Investment_Plans',
              'Label_Wealth_Management_Savings']
Y = df_imputed_original[label_cols].copy()
Y_bin = (Y >= 2).astype(int)

# Prepare user features for tuning.
# Define two feature sets:
base_features = feature_cols.copy()
expanded_features = feature_cols + target_cols  # expanded: include spending categories

# We'll try both feature sets.
feature_sets = {
    'base': base_features,
    'expanded': expanded_features
}

# Split the data into train and test (test set remains unmodified).
X = df_imputed_original.copy()  # full data
X_train_full, X_test, Y_train_bin, Y_test_bin = train_test_split(X, Y_bin, test_size=0.2, random_state=42)

print("\n--- Running grid search for feature set: base ---")
best_params_base, all_results_base = grid_search_cv(base_features, X_train_full, Y_train_bin)
print(f"Best params for base features: {best_params_base}")
print("\n--- Running grid search for feature set: expanded ---")
best_params_expand, all_results_expand = grid_search_cv(expanded_features, X_train_full, Y_train_bin)
print(f"Best params for expanded features: {best_params_expand}")

print(best_params_base)
print(best_params_expand)

def evaluate_lightfm_model(X_train_full, X_test, Y_train_bin, Y_test_bin, feature_set, best_params, label_cols):
    # 1. Feature Selection and Standardization
    X_train_selected = X_train_full[feature_set].copy()
    X_test_selected = X_test[feature_set].copy()

    scaler_final = StandardScaler()
    X_train_final = scaler_final.fit_transform(X_train_selected)
    X_test_final = scaler_final.transform(X_test_selected)

    # Convert to sparse matrices for efficiency
    user_features_train_final = csr_matrix(X_train_final)
    user_features_test_final = csr_matrix(X_test_final)

    # 2. Prepare Interaction Data
    final_interactions_train = csr_matrix(Y_train_bin.values)
    final_interactions_test = csr_matrix(Y_test_bin.values)
    num_items = len(label_cols)
    final_item_features = identity(num_items, format='csr')

    # 3. Train Final Model with Best Parameters
    model_final = LightFM(
        loss=best_params['loss'],
        no_components=best_params['no_components'],
        learning_rate=best_params['learning_rate'],
        user_alpha=best_params['user_alpha'],
        item_alpha=best_params['item_alpha'],
        random_state=42
    )

    model_final.fit(
        final_interactions_train,
        user_features=user_features_train_final,
        item_features=final_item_features,
        epochs=best_params['epochs'],
        num_threads=4
    )

    # 4. Evaluation Metrics
    # Precision@3
    final_precision = precision_at_k(
        model_final,
        final_interactions_test,
        user_features=user_features_test_final,
        item_features=final_item_features,
        k=3
    ).mean()

    # Custom Top-3 Accuracy
    custom_top3_accuracy = compute_top3_accuracy_for_fold(
        model_final,
        user_features_test_final,
        final_interactions_test,
        final_item_features,
        k=3
    )

    # 5. Generate Recommendations (Key Part)
    top3_recommendations = {}
    for user_id in range(user_features_test_final.shape[0]):
        # Predict scores for all items
        scores = model_final.predict(
            user_id,
            np.arange(num_items),
            user_features=user_features_test_final,
            item_features=final_item_features
        )

        # Get indices of top 3 highest scores
        top3_indices = np.argsort(-scores)[:3]

        # Map indices to product names
        recommended_products = [label_cols[idx] for idx in top3_indices]
        top3_recommendations[user_id] = recommended_products

    return {
        'precision@3': final_precision,
        'custom_top3_accuracy': custom_top3_accuracy
    }, top3_recommendations

metrics, recommendations = evaluate_lightfm_model(X_train_full, X_test, Y_train_bin, Y_test_bin,
                                                    base_features, best_params_base, label_cols)
print("\nFinal Model Metrics (using base features):")
for k, v in metrics.items():
    print(f"{k}: {v:.4f}")
print("\nTop 3 product recommendations for sample test users:")
for uid in list(recommendations.keys())[:5]:
    print(f"User {uid}: {recommendations[uid]}")

metrics, recommendations = evaluate_lightfm_model(X_train_full, X_test, Y_train_bin, Y_test_bin,
                                                    expanded_features, best_params_expand, label_cols)
print("\nFinal Model Metrics (using expanded features):")
for k, v in metrics.items():
    print(f"{k}: {v:.4f}")
print("\nTop 3 product recommendations for sample test users:")
for uid in list(recommendations.keys())[:5]:
    print(f"User {uid}: {recommendations[uid]}")

df_imputed_original.shape

"""## Perform PCA on existing data"""

X_expanded = df_imputed_original[expanded_features].copy()
scaler_exp = StandardScaler()
X_expanded_scaled = scaler_exp.fit_transform(X_expanded)

# Assume X_expanded_scaled is already computed (using StandardScaler on your expanded features)
n_total = X_expanded_scaled.shape[1]  # should be 40

for n in range(1, n_total + 1):
    pca_temp = PCA(n_components=n, random_state=42)
    pca_temp.fit(X_expanded_scaled)
    cum_explained = np.sum(pca_temp.explained_variance_ratio_)
    print(f"n_components = {n}: Cumulative explained variance = {cum_explained:.4f}")

pca_no_poly = PCA(n_components=22, random_state=42)  # Adjust n_components as needed.
X_pca = pca_no_poly.fit_transform(X_expanded_scaled)
pca_feature_names = [f'pca_no_poly_{i}' for i in range(X_pca.shape[1])]

df_pca_no_poly = pd.DataFrame(X_pca, columns=pca_feature_names, index=df_imputed_original.index)

final_feature_set = pca_feature_names

# Split the PCA-transformed data and binarized labels into train and test sets.
X_train_pca, X_test_pca, Y_train_bin, Y_test_bin = train_test_split(df_pca_no_poly, Y_bin, test_size=0.2, random_state=42)

print("\n--- Running grid search for PCA features (no polynomial interactions) ---")
best_params_pca, all_results_pca = grid_search_cv(final_feature_set, X_train_pca, Y_train_bin)
print(f"Best PCA params: {best_params_pca}")

metrics_pca, recommendations_pca = evaluate_lightfm_model(
    X_train_pca, X_test_pca, Y_train_bin, Y_test_bin, final_feature_set, best_params_pca, label_cols
)

print("\nFinal Model Metrics using PCA on expanded data (no polynomial interactions):")
for k, v in metrics_pca.items():
    print(f"{k}: {v:.4f}")

print("\nTop 3 product recommendations for sample test users using PCA (no polynomial interactions):")
for uid in list(recommendations_pca.keys())[:5]:
    print(f"User {uid}: {recommendations_pca[uid]}")

poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)
X_poly = poly.fit_transform(X_expanded)
poly_feature_names = poly.get_feature_names_out(expanded_features)

scaler_poly = StandardScaler()
X_poly_scaled = scaler_poly.fit_transform(X_poly)

n_total = X_poly_scaled.shape[1]

for n in range(1, n_total + 1):
    pca_polytemp = PCA(n_components=n, random_state=42)
    pca_polytemp.fit(X_poly_scaled)
    cum_polyexplained = np.sum(pca_polytemp.explained_variance_ratio_)
    print(f"n_components = {n}: Cumulative explained variance = {cum_polyexplained:.4f}")

pcapoly = PCA(n_components=24, random_state=42)
X_poly_pca = pcapoly.fit_transform(X_poly_scaled)
pca_feature_names = [f'pca_{i}' for i in range(X_poly_pca.shape[1])]

# Instead of concatenating with the original expanded features,
# we use only the PCA-transformed polynomial features as the final engineered features.
df_engineered = pd.DataFrame(X_poly_pca, columns=pca_feature_names, index=df_imputed_original.index)
final_feature_set = pca_feature_names

# -----------------------------------------------------------
# Step 5: Split the engineered data and binarized labels into train and test sets.
X_train_eng_full, X_test_eng, Y_train_bin, Y_test_bin = train_test_split(
    df_engineered, Y_bin, test_size=0.2, random_state=42
)

print("\n--- Running grid search for engineered features (using only PCA components) ---")
best_params_eng, all_results_eng = grid_search_cv(final_feature_set, X_train_eng_full, Y_train_bin)
print(f"Best engineered params: {best_params_eng}")

# Evaluate final model using the engineered feature set.
metrics_eng, recommendations_eng = evaluate_lightfm_model(
    X_train_eng_full, X_test_eng, Y_train_bin, Y_test_bin, final_feature_set, best_params_eng, label_cols
)

print("\nFinal Engineered Model Metrics:")
for metric, value in metrics_eng.items():
    print(f"{metric}: {value:.4f}")

print("\nTop 3 product recommendations for sample test users (Engineered Features):")
for uid in list(recommendations_eng.keys())[:5]:
    print(f"User {uid}: {recommendations_eng[uid]}")

pcapoly = PCA(n_components=53, random_state=42)
X_poly_pca = pcapoly.fit_transform(X_poly_scaled)
pca_feature_names = [f'pca_{i}' for i in range(X_poly_pca.shape[1])]

# Instead of concatenating with the original expanded features,
# we use only the PCA-transformed polynomial features as the final engineered features.
df_engineered = pd.DataFrame(X_poly_pca, columns=pca_feature_names, index=df_imputed_original.index)
final_feature_set = pca_feature_names

# -----------------------------------------------------------
# Step 5: Split the engineered data and binarized labels into train and test sets.
X_train_eng_full, X_test_eng, Y_train_bin, Y_test_bin = train_test_split(
    df_engineered, Y_bin, test_size=0.2, random_state=42
)

print("\n--- Running grid search for engineered features (using only PCA components) ---")
best_params_eng, all_results_eng = grid_search_cv(final_feature_set, X_train_eng_full, Y_train_bin)
print(f"Best engineered params: {best_params_eng}")

# Evaluate final model using the engineered feature set.
metrics_eng, recommendations_eng = evaluate_lightfm_model(
    X_train_eng_full, X_test_eng, Y_train_bin, Y_test_bin, final_feature_set, best_params_eng, label_cols
)

print("\nFinal Engineered Model Metrics:")
for metric, value in metrics_eng.items():
    print(f"{metric}: {value:.4f}")

print("\nTop 3 product recommendations for sample test users (Engineered Features):")
for uid in list(recommendations_eng.keys())[:5]:
    print(f"User {uid}: {recommendations_eng[uid]}")